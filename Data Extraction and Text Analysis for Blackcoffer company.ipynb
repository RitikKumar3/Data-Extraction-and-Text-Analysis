{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Data Extraction and Text Analysis for Blackcoffer company"],"metadata":{"id":"Dz-pYc34bdL3"}},{"cell_type":"markdown","source":["## Mounting Google Drive:"],"metadata":{"id":"anFCBEyyX2Zh"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWSP2RSeH439","executionInfo":{"status":"ok","timestamp":1720896743729,"user_tz":-330,"elapsed":8925,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}},"outputId":"667e2d4d-3617-454e-9e57-e3d5419f5b11"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## Importing Necessary Packages:"],"metadata":{"id":"N-ClayYyX83n"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lNMH1gfF3cF","outputId":"19cec7a3-9db0-41c9-83cf-bba1e30ad366","executionInfo":{"status":"ok","timestamp":1720896746561,"user_tz":-330,"elapsed":2836,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Reading Input Excel File:"],"metadata":{"id":"amu6qmR0YBxf"}},{"cell_type":"code","source":["#read the url file into the pandas object\n","df = pd.read_excel('/content/drive/MyDrive/Test Assignment/Input.xlsx')"],"metadata":{"id":"2kjz_6meaqX-","executionInfo":{"status":"ok","timestamp":1720896746562,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Fetching Data from URLs and Writing to Text Files:"],"metadata":{"id":"YxII7mrva0kQ"}},{"cell_type":"code","source":["#loop throgh each row in the df\n","for index, row in df.iterrows():\n","  url = row['URL']\n","  url_id = row['URL_ID']\n","\n","  # make a request to url\n","  header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n","  try:\n","    response = requests.get(url,headers=header)\n","  except:\n","    print(\"can't get response of {}\".format(url_id))\n","\n","  #create a beautifulsoup object\n","  try:\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","  except:\n","    print(\"can't get page of {}\".format(url_id))\n","  #find title\n","  try:\n","    title = soup.find('h1').get_text()\n","  except:\n","    print(\"can't get title of {}\".format(url_id))\n","    continue\n","  #find text\n","  article = \"\"\n","  try:\n","    for p in soup.find_all('p'):\n","      article += p.get_text()\n","  except:\n","    print(\"can't get text of {}\".format(url_id))\n","\n","  #write title and text to the file\n","  file_name = '/content/drive/MyDrive/Test Assignment/TitleText/' + str(url_id) + '.txt'\n","  with open(file_name, 'w') as file:\n","    file.write(title + '\\n' + article)"],"metadata":{"id":"AXXMNkOohRgr","executionInfo":{"status":"ok","timestamp":1720896815636,"user_tz":-330,"elapsed":69078,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Loading Stopwords and Text Files:"],"metadata":{"id":"INIbu0iIYj5p"}},{"cell_type":"code","source":["import os\n","from nltk.tokenize import word_tokenize\n","\n","# Directories\n","text_dir = \"/content/drive/MyDrive/Test Assignment/TitleText\"\n","stopwords_dir = \"/content/drive/MyDrive/Test Assignment/StopWords\"\n","sentiment_dir = \"/content/drive/MyDrive/Test Assignment/MasterDictionary\"\n","\n","# Load all stop words from the stopwords directory and store in the set variable\n","stop_words = set()\n","for filename in os.listdir(stopwords_dir):\n","    file_path = os.path.join(stopwords_dir, filename)\n","    if os.path.isfile(file_path):  # Ensure it's a file, not a directory\n","        with open(file_path, 'r', encoding='ISO-8859-1') as f:\n","            stop_words.update(set(f.read().splitlines()))\n","\n","# Load all text files from the TitleText directory and store in a list (docs)\n","docs = []\n","for text_file in os.listdir(text_dir):\n","    file_path = os.path.join(text_dir, text_file)\n","    if os.path.isfile(file_path):  # Ensure it's a file, not a directory\n","        with open(file_path, 'r') as f:\n","            text = f.read()\n","            words = word_tokenize(text)\n","            filtered_text = [word for word in words if word.lower() not in stop_words]\n","            docs.append(filtered_text)\n","\n"],"metadata":{"id":"YMRtaH22UaI0","executionInfo":{"status":"ok","timestamp":1720896817467,"user_tz":-330,"elapsed":1835,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Sentiment Scores:\n","\n"],"metadata":{"id":"p1JL_J8uaVAa"}},{"cell_type":"code","source":["# Store positive and negative words from the sentiment directory\n","pos = set()\n","neg = set()\n","\n","for filename in os.listdir(sentiment_dir):\n","    file_path = os.path.join(sentiment_dir, filename)\n","    if os.path.isfile(file_path):  # Ensure it's a file, not a directory\n","        if filename == 'positive-words.txt':\n","            with open(file_path, 'r', encoding='ISO-8859-1') as f:\n","                pos.update(f.read().splitlines())\n","        else:\n","            with open(file_path, 'r', encoding='ISO-8859-1') as f:\n","                neg.update(f.read().splitlines())\n","\n","# Initialize lists to store scores\n","positive_words = []\n","Negative_words =[]\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []\n","\n","# Iterate through the list of docs to calculate scores\n","for doc in docs:\n","    positive_words = [word for word in doc if word.lower() in pos]\n","    negative_words = [word for word in doc if word.lower() in neg]\n","    positive_score.append(len(positive_words))\n","    negative_score.append(len(negative_words))\n","    total_words = len(doc)\n","    polarity_score.append((len(positive_words) - len(negative_words)) / (total_words + 0.000001))\n","    subjectivity_score.append((len(positive_words) + len(negative_words)) / (total_words + 0.000001))\n","\n","# Now we have calculated our scores and stored in positive_score, negative_score, polarity_score, and subjectivity_score lists."],"metadata":{"id":"yVu9kz6faQnD","executionInfo":{"status":"ok","timestamp":1720896817467,"user_tz":-330,"elapsed":4,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Text Analysis Metrics (Average Sentence Length, Complex Word Percentage, Fog Index, etc.):"],"metadata":{"id":"lnUroEltacU7"}},{"cell_type":"code","source":["# Average Sentence Length = the number of words / the number of sentences\n","# Percentage of Complex words = the number of complex words / the number of words\n","# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","\n","avg_sentence_length = []\n","Percentage_of_Complex_words  =  []\n","Fog_Index = []\n","complex_word_count =  []\n","avg_syllable_word_count =[]\n","\n","stopwords = set(stopwords.words('english'))\n","def measure(file):\n","  with open(os.path.join(text_dir, file),'r') as f:\n","    text = f.read()\n","# remove punctuations\n","    text = re.sub(r'[^\\w\\s.]','',text)\n","# split the given text file into sentences\n","    sentences = text.split('.')\n","# total number of sentences in a file\n","    num_sentences = len(sentences)\n","# total words in the file\n","    words = [word  for word in text.split() if word.lower() not in stopwords ]\n","    num_words = len(words)\n","\n","# complex words having syllable count is greater than 2\n","# Complex words are words in the text that contain more than two syllables.\n","    complex_words = []\n","    for word in words:\n","      vowels = 'aeiou'\n","      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","      if syllable_count_word > 2:\n","        complex_words.append(word)\n","\n","# Syllable Count Per Word\n","# We count the number of Syllables in each word of the text by counting the vowels present in each word.\n","#  We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n","    syllable_count = 0\n","    syllable_words =[]\n","    for word in words:\n","      if word.endswith('es'):\n","        word = word[:-2]\n","      elif word.endswith('ed'):\n","        word = word[:-2]\n","      vowels = 'aeiou'\n","      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","      if syllable_count_word >= 1:\n","        syllable_words.append(word)\n","        syllable_count += syllable_count_word\n","\n","\n","    avg_sentence_len = num_words / num_sentences\n","    avg_syllable_word_count = syllable_count / len(syllable_words)\n","    Percent_Complex_words  =  len(complex_words) / num_words\n","    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n","\n","    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\n","\n","# iterate through each file or doc\n","for file in os.listdir(text_dir):\n","  x,y,z,a,b = measure(file)\n","  avg_sentence_length.append(x)\n","  Percentage_of_Complex_words.append(y)\n","  Fog_Index.append(z)\n","  complex_word_count.append(a)\n","  avg_syllable_word_count.append(b)"],"metadata":{"id":"F8RaMuD_EnQQ","executionInfo":{"status":"ok","timestamp":1720896818202,"user_tz":-330,"elapsed":738,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Word Count and Average Word Length:"],"metadata":{"id":"EMfsU3HlZh30"}},{"cell_type":"code","source":["# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n","# We count the total cleaned words present in the text by\n","# removing the stop words (using stopwords class of nltk package).\n","# removing any punctuations like ? ! , . from the word before counting.\n","\n","def cleaned_words(file):\n","  with open(os.path.join(text_dir,file), 'r') as f:\n","    text = f.read()\n","    text = re.sub(r'[^\\w\\s]', '' , text)\n","    words = [word  for word in text.split() if word.lower() not in stopwords]\n","    length = sum(len(word) for word in words)\n","    average_word_length = length / len(words)\n","  return len(words),average_word_length\n","\n","word_count = []\n","average_word_length = []\n","for file in os.listdir(text_dir):\n","  x, y = cleaned_words(file)\n","  word_count.append(x)\n","  average_word_length.append(y)\n","\n"],"metadata":{"id":"4NElx7d94ICm","executionInfo":{"status":"ok","timestamp":1720896818202,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Personal Pronouns:"],"metadata":{"id":"5Bw3aSOUZlck"}},{"cell_type":"code","source":["# To calculate Personal Pronouns mentioned in the text, we use regex to find\n","# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n","#  so that the country name US is not included in the list.\n","def count_personal_pronouns(file):\n","  with open(os.path.join(text_dir,file), 'r') as f:\n","    text = f.read()\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    count = 0\n","    for pronoun in personal_pronouns:\n","      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n","  return count\n","\n","pp_count = []\n","for file in os.listdir(text_dir):\n","  x = count_personal_pronouns(file)\n","  pp_count.append(x)"],"metadata":{"id":"kSopsrl6ZbaC","executionInfo":{"status":"ok","timestamp":1720896818813,"user_tz":-330,"elapsed":614,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Modifying Output DataFrame"],"metadata":{"id":"9sPJ4yzCY7co"}},{"cell_type":"code","source":["# Load the existing output DataFrame\n","output_df = pd.read_excel('/content/drive/MyDrive/Test Assignment/Output Data Structure.xlsx')\n","\n","# Drop rows with URL_IDs 44, 57, 144 that couldn't be accessed (adjust index based on your dataset)\n","output_df.drop([44-37, 57-37, 144-37], axis=0, inplace=True)\n","\n","# Verify the number of rows in output_df\n","print(f\"Number of rows after dropping: {len(output_df)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiP-4RIQXOXu","executionInfo":{"status":"ok","timestamp":1720896818814,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}},"outputId":"af0789c7-5664-4ab5-a68e-16ecd4e79c98"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows after dropping: 144\n"]}]},{"cell_type":"markdown","source":["## Saving Output Data to CSV:"],"metadata":{"id":"LA_jdzibZ3B_"}},{"cell_type":"code","source":["# Assign calculated variables to the output DataFrame, ensuring lengths match\n","output_df['AVG_SENTENCE_LENGTH'] = avg_sentence_length[:len(output_df)]\n","output_df['PERCENTAGE_OF_COMPLEX_WORDS'] = Percentage_of_Complex_words[:len(output_df)]\n","output_df['FOG_INDEX'] = Fog_Index[:len(output_df)]\n","output_df['COMPLEX_WORD_COUNT'] = complex_word_count[:len(output_df)]\n","output_df['WORD_COUNT'] = word_count[:len(output_df)]\n","output_df['AVG_SYLLABLES_PER_WORD'] = avg_syllable_word_count[:len(output_df)]\n","output_df['PERSONAL_PRONOUNS'] = pp_count[:len(output_df)]\n","output_df['AVG_WORD_LENGTH'] = average_word_length[:len(output_df)]\n","\n","# Save the DataFrame to a CSV file\n","output_df.to_csv('/content/drive/MyDrive/Test Assignment/Output_Data.csv', index=False)\n","\n","print(\"Output data saved successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rwY3S29RZuwV","executionInfo":{"status":"ok","timestamp":1720896818814,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ritik Kumar","userId":"05677812184007054339"}},"outputId":"7db4570e-343e-4d7f-9d0a-61925d2b7e78"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Output data saved successfully.\n"]}]}]}